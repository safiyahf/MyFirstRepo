{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safiyahf/MyFirstRepo/blob/main/custom_finetuning_workshop_student_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff3930e2",
      "metadata": {
        "id": "ff3930e2"
      },
      "source": [
        "# Fine-tuning Llama 3.2:3B with Unsloth on Custom Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "861ff518",
      "metadata": {
        "id": "861ff518"
      },
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45165a6b",
      "metadata": {
        "id": "45165a6b"
      },
      "source": [
        "This step prepares your Google Colab environment by installing Unsloth and its dependencies. Unsloth is a library that optimizes LLM training to be faster and more memory-efficient. The installation code detects if you're running in Colab and installs the appropriate dependencies. This is essential groundwork that enables the rest of the fine-tuning process to run smoothly on Google's free GPU resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8538cd33",
      "metadata": {
        "id": "8538cd33"
      },
      "outputs": [],
      "source": [
        "# Install Unsloth and dependencies\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers peft==0.14.0 trl==0.16.0 triton==3.2.0\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo==2025.3.16\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth==2025.3.18\n",
        "    !pip install transformers==4.49.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f291ee14",
      "metadata": {
        "id": "f291ee14"
      },
      "source": [
        "## Initialize Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4660f917",
      "metadata": {
        "id": "4660f917"
      },
      "source": [
        "Here, you load a pre-trained language model (Llama 3.2 3B Instruct) that will serve as the foundation for fine-tuning. The code configures important parameters like sequence length and sets up memory-efficient 4-bit quantization to reduce VRAM usage. It also initializes LoRA (Low-Rank Adaptation) adapters, which is a technique that allows you to fine-tune only a small subset of the model's parameters (typically 1-10%), dramatically reducing memory requirements and training time while maintaining performance.\n",
        "\n",
        "Before you run this block make sure the colab runtime is on a GPU. To do this:\n",
        "\n",
        "1)Click on \"Runtime\" in the top menu\n",
        "\n",
        "2)Select \"Change runtime type\"\n",
        "\n",
        "2)In the dialog that appears, make sure T4 GPU is selected\n",
        "\n",
        "3)Click \"Save\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "210b71bc",
      "metadata": {
        "id": "210b71bc"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Configure model parameters\n",
        "max_seq_length = 2048  # Choose any length (auto RoPE scaling supported)\n",
        "dtype = None  # None for auto detection. Float16 for T4, V100. Bfloat16 for Ampere+ GPUs\n",
        "load_in_4bit = True  # Use 4-bit quantization to reduce memory usage\n",
        "\n",
        "# Load the base model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name =,  # You can change this to other models\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\",  # Uncomment and add your token if using gated models\n",
        ")\n",
        "\n",
        "# Add LoRA adapters to update only a small percentage of parameters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r =,  # Rank: Choose 8, 16, 32, 64, or 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,  # 0 is optimized\n",
        "    bias = \"none\",     # \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Use \"unsloth\" for 30% less VRAM\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # Rank stabilized LoRA (optional)\n",
        "    loftq_config = None,  # LoftQ (optional)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee7958fe",
      "metadata": {
        "id": "ee7958fe"
      },
      "source": [
        "## Set Up Chat Template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727ac4dd",
      "metadata": {
        "id": "727ac4dd"
      },
      "source": [
        "This step configures how conversations are formatted for the model. The chat template structures the input text with special tokens that help the model distinguish between user queries and assistant responses. For Llama 3.1, this includes specific header markers like <|start_header_id|>user<|end_header_id|>. Proper formatting is crucial because it teaches the model to recognize the conversation structure and generate appropriate responses in the correct style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b098908",
      "metadata": {
        "id": "1b098908"
      },
      "outputs": [],
      "source": [
        "# Configure the chat template for the tokenizer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.2\",  # You can also use: zephyr, chatml, mistral, vicuna, etc.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a911a8",
      "metadata": {
        "id": "b3a911a8"
      },
      "source": [
        "## Prepare Custom Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6adf9e9",
      "metadata": {
        "id": "f6adf9e9"
      },
      "source": [
        "This critical step processes your training data into the format required by the model. Three different methods are provided: manually creating a dataset structure with example conversations, loading from a CSV file, or importing from Hugging Face. The data needs to be structured as conversations with alternating user and assistant messages. The code then applies the chat template to format each conversation properly and converts it into tokenized inputs that the model can process during training. We shall be using our brainrot dataset to increase the LLM's rizz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4adc437",
      "metadata": {
        "id": "a4adc437"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# OPTION 1: Create a custom dataset from a DataFrame\n",
        "# This is an example - replace with your actual data loading code\n",
        "\n",
        "# Example: Create a simple dataset with 3 samples\n",
        "\"\"\"\n",
        "data = {\n",
        "    \"conversations\": [\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\"}\n",
        "        ],\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": \"Explain neural networks simply\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by the human brain. They consist of interconnected nodes (neurons) that process and transmit information, allowing the system to learn patterns from data.\"}\n",
        "        ],\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": \"How does reinforcement learning work?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. It's based on trial and error, with feedback in the form of rewards or penalties.\"}\n",
        "        ]\n",
        "    ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# OPTION 2: Load data from a CSV file - Uncomment to use\n",
        "# Assuming your CSV has columns: 'user_message' and 'assistant_response'\n",
        "\"\"\"\n",
        "df = pd.read_csv()\n",
        "\n",
        "# Convert your data to the expected format\n",
        "conversations = []\n",
        "for i in range(len(df)):\n",
        "    convo = [\n",
        "        {\"role\": \"user\", \"content\": df.loc[i, 'user_message']},\n",
        "        {\"role\": \"assistant\", \"content\": df.loc[i, 'assistant_response']}\n",
        "    ]\n",
        "    conversations.append(convo)\n",
        "\n",
        "data = {\"conversations\": conversations}\n",
        "\"\"\"\n",
        "\n",
        "# OPTION 3: Load from a Hugging Face dataset - Uncomment to use\n",
        "\"\"\"\n",
        "from datasets import load_dataset\n",
        "external_dataset = load_dataset(\"your_username/your_dataset\", split=\"train\")\n",
        "\n",
        "# If your dataset is already in the right format, you can use it directly\n",
        "# Otherwise, you'll need to convert it to the proper format with conversations\n",
        "\"\"\"\n",
        "# Create the dataset from your data\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "# Format the data for training\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Show an example of the formatted data\n",
        "print(\"Example of formatted data:\")\n",
        "print(dataset[0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef134d83",
      "metadata": {
        "id": "ef134d83"
      },
      "source": [
        "## Configure Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b57617",
      "metadata": {
        "id": "79b57617"
      },
      "source": [
        "Here, you set up the SFTTrainer (Supervised Fine-Tuning Trainer) with parameters that control the learning process. This includes batch size, learning rate, number of epochs, and optimizer settings. The code also configures a crucial optimization that masks the loss calculation so that the model only learns from assistant responses, not from user inputs. This step essentially defines how the model will learn from your data and how quickly it will adapt to generate responses in the style of your training examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6003ad31",
      "metadata": {
        "id": "6003ad31"
      },
      "outputs": [],
      "source": [
        "# Set up the trainer\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "# Configure training arguments\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # Set to True for 5x faster training with short sequences\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=3,  # Adjust based on your dataset size\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # Set to \"wandb\" for Weights & Biases logging\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Configure the trainer to only train on assistant responses\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\",\n",
        "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
        ")\n",
        "\n",
        "# Verify masking is correctly applied\n",
        "print(\"Verifying masking:\")\n",
        "print(tokenizer.decode(trainer.train_dataset[0][\"input_ids\"]))\n",
        "\n",
        "space = tokenizer(\" \", add_special_tokens=False).input_ids[0]\n",
        "print(tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa857f60",
      "metadata": {
        "id": "aa857f60"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e1b507",
      "metadata": {
        "id": "c6e1b507"
      },
      "source": [
        "This step executes the actual training process and monitors GPU resource usage. The model begins learning from your dataset, updating the LoRA parameters to better match the style and content of the assistant responses in your training data. The code tracks metrics like training time and memory usage, which helps you understand the resource requirements and efficiency of your fine-tuning job. For larger datasets, this step may take considerable time, but the optimizations from Unsloth make it much faster than traditional methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e635ed7d",
      "metadata": {
        "id": "e635ed7d"
      },
      "outputs": [],
      "source": [
        "# Display GPU information\n",
        "print(\"GPU Information:\")\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Display training statistics\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "751584d2",
      "metadata": {
        "id": "751584d2"
      },
      "source": [
        "## Inference with Fine-tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f18cf1e",
      "metadata": {
        "id": "6f18cf1e"
      },
      "source": [
        "After training, this step lets you immediately test your fine-tuned model with a sample prompt. The model is switched to inference mode for faster generation, and a TextStreamer is set up to display the generated response token by token. This gives you immediate feedback on how well your model has learned from the training data and allows you to assess if it needs further training or adjustments before final deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a21096",
      "metadata": {
        "id": "11a21096"
      },
      "outputs": [],
      "source": [
        "# Set up the model for inference\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "# Test the model with a prompt\n",
        "test_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"}\n",
        "]\n",
        "\n",
        "# Generate a response\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    test_messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Stream the output\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38fdcef1",
      "metadata": {
        "id": "38fdcef1"
      },
      "source": [
        "## Save the Fine-tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44a8f97",
      "metadata": {
        "id": "b44a8f97"
      },
      "source": [
        "This final step preserves your trained model for future use. You can save just the LoRA adapters (which are much smaller than the full model), push the model to Hugging Face Hub for sharing, or convert it to different formats like GGUF for use with llama.cpp. These options give you flexibility in how you deploy your model, whether for personal use, collaboration with others, or integration into different applications and platforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2bff51",
      "metadata": {
        "id": "ae2bff51"
      },
      "outputs": [],
      "source": [
        "# Save the LoRA adapters (smaller size, requires base model to use)\n",
        "model.save_pretrained(\"finetuned_lora_model\")\n",
        "tokenizer.save_pretrained(\"finetuned_lora_model\")\n",
        "\n",
        "# Optional: Push to Hugging Face Hub\n",
        "# model.push_to_hub(\"your_username/your_model_name\", token=\"your_hf_token\")\n",
        "# tokenizer.push_to_hub(\"your_username/your_model_name\", token=\"your_hf_token\")\n",
        "\n",
        "# Optional: Save as merged model in float16 (full model, larger size)\n",
        "# model.save_pretrained_merged(\"finetuned_merged_model\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# Optional: Save as GGUF for llama.cpp\n",
        "# model.save_pretrained_gguf(\"finetuned_gguf_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "\n",
        "print(\"Fine-tuning completed and model saved!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}